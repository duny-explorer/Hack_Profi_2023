{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DmBBdW2SClvB"
   },
   "outputs": [],
   "source": [
    "import openfoamparser_mai as Ofpp\n",
    "import os\n",
    "import numpy as np\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.palettes import Spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XP-y0cdbL4xG"
   },
   "outputs": [],
   "source": [
    "data_V = []\n",
    "target_V = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RvjKtRjIKztX"
   },
   "outputs": [],
   "source": [
    "for directory in sorted(os.listdir('data_wage\\\\data_wage\\\\low_dim\\\\')):\n",
    "    data_dir = None\n",
    "    target_dir = None\n",
    "    \n",
    "    for file in sorted(os.listdir('data_wage\\\\data_wage\\\\low_dim\\\\' + directory)):\n",
    "        if file not in ['constant', 'system', '0']:\n",
    "            if data_dir is not None:\n",
    "                data_dir = np.append(data_dir, np.array([Ofpp.parse_internal_field('data_wage\\\\data_wage\\\\low_dim\\\\' \\\n",
    "                                                                               + directory + '\\\\' + file + '\\\\U')]), axis=0)\n",
    "                target_dir = np.append(target_dir, np.array([Ofpp.parse_internal_field('data_wage\\\\data_wage\\\\high_dim\\\\' \\\n",
    "                                                                               + directory + '\\\\' + file + '\\\\U')]), axis=0)\n",
    "            else:\n",
    "                data_dir = np.array([Ofpp.parse_internal_field('data_wage\\\\data_wage\\\\low_dim\\\\' \\\n",
    "                                                             + directory + '\\\\' + file + '\\\\U')])\n",
    "                target_dir = np.array([Ofpp.parse_internal_field('data_wage\\\\data_wage\\\\high_dim\\\\' \\\n",
    "                                                             + directory + '\\\\' + file + '\\\\U')])\n",
    "        \n",
    "    data_V.append(data_dir)\n",
    "    target_V.append(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DpF2IedMKUOE"
   },
   "outputs": [],
   "source": [
    "low_num_nodes = 75\n",
    "def create_low_dim_graph(path, features):\n",
    "    mesh = Ofpp.FoamMesh(path)\n",
    "\n",
    "    neighbours = []\n",
    "\n",
    "    for i in range(low_num_nodes):\n",
    "        for j in list(filter(lambda x: 0 <= x < low_num_nodes, mesh.cell_neighbour_cells(i))):\n",
    "            neighbours.append((i, j))\n",
    "\n",
    "    g = dgl.graph(neighbours)\n",
    "    g.ndata[\"attr\"] = torch.from_numpy(features).float()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YE9ZQx8JZopH",
    "outputId": "3bce577c-71f4-4b7f-bd2a-b2a3cab3eed6"
   },
   "outputs": [],
   "source": [
    "DIR_PATH = 'data_wage\\\\data_wage\\\\low_dim\\\\'\n",
    "data = []\n",
    "\n",
    "for i, directory in enumerate(sorted(os.listdir(DIR_PATH))):\n",
    "    for j in range(10):\n",
    "        data.append(create_low_dim_graph(DIR_PATH + directory + \"\\\\\", data_V[i][j][:, :3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gBFszSidaIg8"
   },
   "outputs": [],
   "source": [
    "high_num_nodes = 4800\n",
    "\n",
    "def create_high_dim_graph(path, features):\n",
    "    mesh = Ofpp.FoamMesh(path)\n",
    "\n",
    "    neighbours = []\n",
    "\n",
    "    for i in range(high_num_nodes):\n",
    "        for j in list(filter(lambda x: 0 <= x < high_num_nodes, mesh.cell_neighbour_cells(i))):\n",
    "            neighbours.append((i, j))\n",
    "\n",
    "    g = dgl.graph(neighbours)\n",
    "    g.ndata[\"attr\"] = torch.from_numpy(features).float()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:23,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "DIR_PATH = 'data_wage\\\\data_wage\\\\high_dim\\\\'\n",
    "target = []\n",
    "\n",
    "for i, directory in tqdm(enumerate(sorted(os.listdir(DIR_PATH)))):\n",
    "    for j in range(10):\n",
    "        target.append(create_high_dim_graph(DIR_PATH + directory + \"\\\\\", target_V[i][j][:, :3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8687e+00, -7.4313e-15,  0.0000e+00],\n",
       "        [ 5.8687e+00, -2.2237e-14,  0.0000e+00],\n",
       "        [ 5.8687e+00, -2.6675e-14,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 5.8687e+00,  2.1250e-13,  0.0000e+00],\n",
       "        [ 5.8687e+00,  1.6029e-13,  0.0000e+00],\n",
       "        [ 5.8687e+00, -1.7198e-14,  0.0000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].ndata['attr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = y_train[1].edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_75 = X_train[1].edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mildarnikitin20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import dgl\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\daled\\forum\\wandb\\run-20230426_124643-pp3ojcnk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ildarnikitin20/uncategorized/runs/pp3ojcnk' target=\"_blank\">autumn-firebrand-2</a></strong> to <a href='https://wandb.ai/ildarnikitin20/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ildarnikitin20/uncategorized' target=\"_blank\">https://wandb.ai/ildarnikitin20/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ildarnikitin20/uncategorized/runs/pp3ojcnk' target=\"_blank\">https://wandb.ai/ildarnikitin20/uncategorized/runs/pp3ojcnk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ildarnikitin20/uncategorized/runs/pp3ojcnk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x15f1c9c5040>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:35<00:00, 25.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 66.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.9355, g_loss: 4.7665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:28<00:00, 31.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 64.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.6813, g_loss: 0.2174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:28<00:00, 31.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 68.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.4577, g_loss: 0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:29<00:00, 30.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:12<00:00, 69.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.2837, g_loss: 0.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:30<00:00, 29.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 65.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.1789, g_loss: 0.0415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:29<00:00, 30.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 67.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.1119, g_loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:29<00:00, 30.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 68.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0707, g_loss: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:27<00:00, 32.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 68.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0441, g_loss: 0.0235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:29<00:00, 30.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 66.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0279, g_loss: 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:28<00:00, 31.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:15<00:00, 58.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0172, g_loss: 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:27<00:00, 32.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 65.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0106, g_loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:28<00:00, 31.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 66.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0066, g_loss: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:29<00:00, 30.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 64.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0042, g_loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:28<00:00, 31.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 66.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0026, g_loss: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:28<00:00, 31.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:13<00:00, 68.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.0017, g_loss: 0.0144\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = GraphConv(3, 256)\n",
    "        self.conv2 = GraphConv(256, 4800 * 3)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = self.conv1(g, g.ndata['attr'])\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h).view(-1, 4800, 3)\n",
    "        h = h.mean(dim=0)\n",
    "#         print(h.shape)\n",
    "        new_g = dgl.graph(edge_index).to(device)\n",
    "        new_g.ndata['attr'] = h.to(device)\n",
    "        return new_g\n",
    "\n",
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = GraphConv(3, 16)\n",
    "        self.conv2 = GraphConv(16, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['attr']\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = self.conv2(g, h)\n",
    "        h = h.mean(dim=0)\n",
    "\n",
    "        h = torch.sigmoid(h)\n",
    "        return h\n",
    "    \n",
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "\n",
    "# Define the training loop\n",
    "criterion = nn.BCELoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0005)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0005)\n",
    "\n",
    "for epoch in range(15):\n",
    "    all_d_loss = 0\n",
    "    all_g_loss = 0\n",
    "\n",
    "    for i in tqdm(range(len(X_train))):\n",
    "        # Train the discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        real_data = y_train[i]\n",
    "        real_label = torch.ones_like(discriminator(real_data.to(device)))\n",
    "        fake_graph_features = torch.randn(75, 3)\n",
    "        fake_graph = dgl.graph(edge_index_75).to(device)\n",
    "        fake_graph.ndata['attr'] = fake_graph_features.to(device)\n",
    "        fake_data = generator(fake_graph)\n",
    "        fake_label = torch.zeros_like(discriminator(fake_data))\n",
    "\n",
    "        d_loss_real = criterion(discriminator(real_data.to(device)), real_label)\n",
    "        d_loss_fake = criterion(discriminator(fake_data.to(device)), fake_label)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        all_d_loss += d_loss \n",
    "    all_d_loss /= (i + 1)\n",
    "    \n",
    "    for i in tqdm(range(len(X_train))):\n",
    "        # Train the generator\n",
    "        generator.zero_grad()\n",
    "        \n",
    "        fake_data = generator(X_train[i].to(device))\n",
    "        g_loss = criterion(discriminator(fake_data), real_label)\n",
    "        p_loss = criterion2(y_train[i].ndata['attr'].to(device), fake_data.ndata['attr'].to(device))\n",
    "        g_loss += p_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        all_g_loss += g_loss \n",
    "    all_g_loss /= (i + 1)\n",
    "\n",
    "        # Output training stats\n",
    "    if epoch % 1 == 0:\n",
    "        print('d_loss: {:.4f}, g_loss: {:.4f}'.format(all_d_loss, all_g_loss))\n",
    "        record = {\n",
    "            'd_loss': all_d_loss,\n",
    "            'g_loss': all_g_loss\n",
    "        }\n",
    "        wandb.log(record)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Генерация графов повышенного разрешения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 78.48it/s]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    result.append(generator(X_test[i].to(device)).ndata['attr'].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.from_numpy(np.array(result).reshape((100, 4800, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for g in y_test:\n",
    "    targets.append(g.ndata['attr'].cpu().detach().numpy())\n",
    "    \n",
    "targets = torch.from_numpy(np.array(targets).reshape((100, 4800, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4958)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "mean_abs_percentage_error = MeanAbsolutePercentageError()\n",
    "mean_abs_percentage_error(targets[:, :, :2], result[:, :, :2])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
